<style>
p {
  text-indent: 20px;
}
li {
  text-indent: 20px;
}
</style>

<h1>Maximum Likelihood Estimation</h1> 

The animation (generated with <a
    href="http://www.cinderella.de">Cinderella</a>) visualizes the effect of the covariance matrix of observations onto the Maximum Likelihood estimation of a parameter.
		
<p>  The animation shows the estimation of the single variable &theta; lying on a line (white), i.e. we assume E(y)=x&theta;, with the design matrix X=x=[x<sub>1</sub>,x<sub>2</sub>], |x|=1, based on the observed point y=[y<sub>1</sub>,y<sub>2</sub>]), not sitting on the line. The simple least squares solution is the point x&theta;|I<sub>2</sub> being the point on the line closest to y. In case the two coordinates y<sub>1</sub> and y<sub>2</sub> have the joint covariance matrix &Sigma; then the best point is x&theta;|&Sigma;. The tangent point T of the (yellow) tangent at the ellipse (representing &Sigma;) which is parallel to the line yields the direction from y to the optimal point.  </p>
    
<p> You may change the configuration by moving the white line on x or the point y. The semi-axes s<sub>1</sub> and s<sub>2</sub> of the covariance matrix can be changed by moving the red points in the left upper corner. The direction of the major semiaxis can be changed by rotating the red arrow at M. Changing the covariance matrix changes the estimated point. You may adapt the notation, substituting (&theta;,X,y) by (x,A,l).</p>


<p>  Generally, changing the three parameters of the covariance matrix also lead to changes of the estimated point x&theta;|&Sigma;. The difference to the simple least squares solution with &Sigma;=I<sub>2</sub> only stays unchanged, if the major semiaxis of &Sigma; is parallel or perpendicular to the direction x. So: Rotate the ellipse such that a semiaxis is parallel to x, then changing the semiaxes s<sub>1</sub> or s<sub>2</sub> does not lead to changes of the estimate. This is the result of <a href="https://typeset.io/pdf/least-squares-theory-using-an-estimated-dispersion-matrix-3pqpdbs9lm.pdf#page=10"> Rao's lemma 5a, (1967, p. 10)</a>.</p>


<p> Changing the line to y<sub>2</sub>=y<sub>1</sub>, hence x &prop; [1,1], we can realize the general mean of the two observations. By choosing a point on the y<sub>1</sub>-axis as observation, say y=[3,0], we may be able to find a covariance matrix of the observations which leads to a point y<sub>2</sub>=y<sub>1</sub> outside the range [3,0]. However, 
when assuming no correlation between y<sub>1</sub> and y<sub>2</sub>, i.e. &phi;=0 or &phi;=90&deg;, the mean always stays within the interval [y<sub>1</sub>,y<sub>2</sub>].</p>